{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from fil"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Attention-based Transformer Network\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def get_layer_clones(layer, num_layers):\n",
    "    return [layer for _i in range(num_layers)]\n",
    "\n",
    "\n",
    "class SubLayerConnection(tf.keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(SubLayerConnection, self).__init__()\n",
    "        self.norm_layer = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "    def call(self, _input, _residual_input):  # _input is sublayer(x), _residual_input is x\n",
    "        output = self.norm_layer(tf.keras.layers.Add()([_input, _residual_input]))\n",
    "\n",
    "        return output\n",
    "\n",
    "class FeedForwardLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, ffn_units, _d_model, dropout_rate=0.3):\n",
    "        super(FeedForwardLayer, self).__init__()\n",
    "        self.relu = tf.keras.layers.ReLU(ffn_units)\n",
    "        self.ffn = tf.keras.layers.Dense(units=_d_model)\n",
    "        self.drop = tf.keras.layers.Dropout(rate=dropout_rate)\n",
    "        self.norm = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "\n",
    "    def call(self, _inputs):\n",
    "        _x = self.relu(_inputs)\n",
    "        _x = self.drop(_x)\n",
    "        output = self.norm(_x)\n",
    "\n",
    "        return output\n",
    "\n",
    "class EncoderLayer(tf.keras.layers.Layer): # Comprises of multi-headed self-attention & feed-forward\n",
    "    def __init__(self, num_heads, key_dim, _d_model, _ffn_units):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.d_model = _d_model\n",
    "        self.multi_head_attention = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=key_dim)\n",
    "        self.feed_forward_layer = FeedForwardLayer(_d_model=self.d_model, ffn_units=_ffn_units)\n",
    "\n",
    "        self.subconn_layers = get_layer_clones(layer=SubLayerConnection(), num_layers=2)\n",
    "\n",
    "    def call(self, _inputs, _mask):\n",
    "        print(f\"encoderLayer mha query shape: {_inputs.shape}\")\n",
    "        self_attn = self.multi_head_attention(query=_inputs,\n",
    "                                              value=_inputs,\n",
    "                                              key = _inputs,\n",
    "                                              return_attention_scores=False,\n",
    "                                              training=False,\n",
    "                                              attention_mask=_mask)\n",
    "        print(f\"encoder self attn shape: {self_attn.shape}\")\n",
    "        l1 = self.subconn_layers[0](_input=self_attn,\n",
    "                                    _residual_input=_inputs)\n",
    "\n",
    "        print(f\"encoder l1 shape: {l1.shape}\")\n",
    "\n",
    "        output = self.subconn_layers[1](_input=self.feed_forward_layer(l1),\n",
    "                                        _residual_input=l1)\n",
    "\n",
    "        return output\n",
    "\n",
    "class PositionalEncodingLayer(tf.keras.layers.Layer):\n",
    "    \"\"\"code adapted from https://towardsdatascience.com/attention-is-all-you-need-discovering-the-transformer-paper-73e5ff5e0634\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(PositionalEncodingLayer, self).__init__()\n",
    "\n",
    "    @staticmethod\n",
    "    def get_angles(pos, i, _d_model, _n=10000.): # pos: (seq_length, 1) i: (1, d_model)\n",
    "        angles = 1 / np.power(_n, (2*(i//2)) / np.float32(_d_model))\n",
    "        return pos * angles # (seq_length, d_model)\n",
    "\n",
    "    def call(self, _inputs, _d_model, _seq_len, _type):\n",
    "\n",
    "        # input shape batch_size, seq_length, d_model\n",
    "\n",
    "        print(f\"_inputs shape in pos_enc: {_inputs.shape}\")\n",
    "        # Calculate the angles given the input\n",
    "        angles = self.get_angles(np.arange(_seq_len)[:, np.newaxis],\n",
    "                                 np.arange(_d_model)[np.newaxis, :],\n",
    "                                 _d_model)\n",
    "        # Calculate the positional encodings\n",
    "        # apply sin to even indices in the array; 2i\n",
    "        angles[:, 0::2] = np.sin(angles[:, 0::2])\n",
    "\n",
    "        # apply cos to odd indices in the array; 2i+1\n",
    "        angles[:, 1::2] = np.cos(angles[:, 1::2])\n",
    "\n",
    "        if _type == 'encoder':\n",
    "            # Expand the encodings with a new dimension\n",
    "            pos_encoding = angles[np.newaxis, ...]\n",
    "            return _inputs + tf.cast(pos_encoding, tf.float32)\n",
    "        elif _type == 'decoder':\n",
    "            return tf.cast(angles, tf.float32)\n",
    "        else:\n",
    "            raise TypeError('Wrong type specified for PositionalEncodingLayer. Type must be one of \"encoder\" or \"decoder\".')\n",
    "\n",
    "\n",
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, _num_heads, _key_dim, _embedding_size, _d_model, _seq_len, _ffn_units, num_layers=6, _embedding_layer=None):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.d_model = _d_model\n",
    "        self.seq_len = _seq_len\n",
    "\n",
    "        print(f\"size of vocab: {len(tokenizer.word_index)+1}\")\n",
    "\n",
    "        print(f\"_embedding_layer: {_embedding_layer}\")\n",
    "\n",
    "        self._embedding_layer = _embedding_layer\n",
    "\n",
    "        if _embedding_layer is None:\n",
    "            self.word_embedding_layer = tf.keras.layers.Embedding(\n",
    "                input_dim=len(tokenizer.word_index)+1, # size of vocab\n",
    "                output_dim=_d_model, # size of vector space to be embedded in\n",
    "                weights=[get_embedding_vectors(tokenizer, word2embedding_map)],\n",
    "                trainable=False,\n",
    "                input_length=self.seq_len, # length of input seqs\n",
    "        )\n",
    "        else:\n",
    "            self.word_embedding_layer = _embedding_layer\n",
    "\n",
    "        self.positional_encoding_layer = PositionalEncodingLayer()\n",
    "\n",
    "        encoder_layer = EncoderLayer(num_heads=_num_heads,\n",
    "                                     key_dim=_key_dim,\n",
    "                                     _d_model=_d_model,\n",
    "                                     _ffn_units=_ffn_units)\n",
    "\n",
    "        self.layers = get_layer_clones(encoder_layer, num_layers)  # repeat the encoder_layer by num_layers many times\n",
    "\n",
    "    def call(self, _inputs, _mask):\n",
    "        if self._embedding_layer is None:\n",
    "            word_embedding_layer = self.word_embedding_layer(_inputs)\n",
    "        else:\n",
    "            word_embedding_layer = self.word_embedding_layer\n",
    "\n",
    "        print(f\"word embedding shape: {word_embedding_layer.shape}\")\n",
    "\n",
    "        # Scale the embeddings by sqrt of d_model\n",
    "        word_embedding_layer *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "\n",
    "        # Positional Encodings\n",
    "        pos_encoded_inputs = self.positional_encoding_layer(_inputs=word_embedding_layer,\n",
    "                                                            _d_model=self.d_model,\n",
    "                                                            _seq_len=self.seq_len,\n",
    "                                                            _type='encoder')\n",
    "\n",
    "        print(f\"pos_encoded_inputs shape: {pos_encoded_inputs.shape}\")\n",
    "\n",
    "        layer_count = 1\n",
    "\n",
    "        for _layer in self.layers:\n",
    "            if layer_count == 1:\n",
    "                _x = _layer(_inputs=pos_encoded_inputs, _mask=_mask)\n",
    "            else:\n",
    "                _x = _layer(_inputs=_x, _mask=_mask)\n",
    "\n",
    "            print(f\"encoderLayer shape: {_x.shape}\")\n",
    "\n",
    "            layer_count += 1\n",
    "\n",
    "        return _x\n",
    "\n",
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_heads, key_dim, _d_model, _ffn_units):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.masked_multi_head_attention = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=key_dim)\n",
    "        self.multi_head_attention = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=key_dim)\n",
    "        self.feed_forward_layer = FeedForwardLayer(_d_model=_d_model, ffn_units=_ffn_units)\n",
    "\n",
    "        subconn_layer = SubLayerConnection()\n",
    "        self.subconn_layers = get_layer_clones(subconn_layer, 3)\n",
    "    def call(self, _inputs, _encoder_output, _mask):\n",
    "        print(f\"query1: _inputs shape: {_inputs.shape}\")\n",
    "        masked_multihead_attention = self.masked_multi_head_attention(query=_inputs,\n",
    "                                                                      key=_inputs,\n",
    "                                                                      value=_inputs,\n",
    "                                                                      attention_mask=_mask,\n",
    "                                                                      return_attention_scores=False,\n",
    "                                                                      training=True)\n",
    "        l1 = self.subconn_layers[0](_input=masked_multihead_attention,\n",
    "                                    _residual_input=_inputs)\n",
    "        print(f\"masked_multihead_attention shape: {masked_multihead_attention.shape}\")\n",
    "        print(f\"_inputs shape: {_inputs.shape}\")\n",
    "        print(f\"query2: decoder l1 shape: {l1.shape}\")\n",
    "        print(f\"key2, val2: decoder enc_out shape: {_encoder_output.shape}\")\n",
    "        multi_head_attention = self.multi_head_attention(key=_encoder_output,\n",
    "                                                         value=_encoder_output,\n",
    "                                                         query=l1,\n",
    "                                                         return_attention_scores=False,\n",
    "                                                         training=False)\n",
    "        l2 = self.subconn_layers[1](_input=multi_head_attention,\n",
    "                                    _residual_input=l1)\n",
    "        feed_forward = self.feed_forward_layer(_inputs=l2)\n",
    "\n",
    "        l3 = self.subconn_layers[2](_input=feed_forward,\n",
    "                                    _residual_input=l2)\n",
    "\n",
    "        return l3\n",
    "\n",
    "\n",
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, _num_heads, _key_dim, _seq_len, _d_model, _ffn_units):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.seq_len = _seq_len\n",
    "        self.d_model = _d_model\n",
    "\n",
    "        self.positional_encoding_layer = PositionalEncodingLayer()\n",
    "\n",
    "        decoder_layer = DecoderLayer(num_heads=_num_heads,\n",
    "                                     key_dim=_key_dim,\n",
    "                                     _d_model=self.d_model,\n",
    "                                     _ffn_units=_ffn_units)\n",
    "        self.decoder_layers = get_layer_clones(layer=decoder_layer, num_layers=6)\n",
    "\n",
    "\n",
    "    def call(self, _inputs, _encoder_output, _mask):\n",
    "        # Scale Embeddings by sqrt of d_model\n",
    "        _inputs *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "\n",
    "        # Positional encoding\n",
    "        pos_encoded_inputs = self.positional_encoding_layer(_inputs=_inputs,\n",
    "                                                            _d_model=self.d_model,\n",
    "                                                            _seq_len=self.seq_len,\n",
    "                                                            _type='encoder')\n",
    "        print(f\"decoder pos_encoded_inputs shape: {pos_encoded_inputs}\")\n",
    "\n",
    "\n",
    "        layer_count = 1\n",
    "\n",
    "        for _layer in self.decoder_layers:\n",
    "            if layer_count==1:\n",
    "                _x = _layer(_inputs=pos_encoded_inputs,\n",
    "                            _encoder_output=_encoder_output,\n",
    "                            _mask=_mask)\n",
    "            else:\n",
    "                _x = _layer(_inputs=_x,\n",
    "                            _encoder_output=_encoder_output,\n",
    "                            _mask=_mask)\n",
    "\n",
    "            layer_count += 1\n",
    "\n",
    "\n",
    "\n",
    "        return _x\n",
    "\n",
    "class AttentionBasedTransformer(tf.keras.Model):\n",
    "    def __init__(self, _embedding_size, _key_dim, _num_heads, _mask, _batch_size, _input_shape, _mem_matrix, _seq_len, _d_model, _embedding_layer=None):\n",
    "        super(AttentionBasedTransformer, self).__init__()\n",
    "\n",
    "        self.count = 0\n",
    "\n",
    "        self.inputs_shape = _input_shape\n",
    "        self.seq_len = _seq_len\n",
    "        self.d_model = _d_model\n",
    "        self.ffn_units = 64\n",
    "\n",
    "        self.inputs = tf.keras.layers.InputLayer(input_shape=_input_shape, batch_size=_batch_size)\n",
    "\n",
    "        self.encoder = Encoder(_embedding_size=_embedding_size,\n",
    "                               _key_dim=_key_dim,\n",
    "                               _num_heads=_num_heads,\n",
    "                               _seq_len=self.seq_len,\n",
    "                               _d_model=self.d_model,\n",
    "                               _ffn_units=self.ffn_units,\n",
    "                               _embedding_layer=_embedding_layer)\n",
    "        self.mask = _mask\n",
    "\n",
    "        print(f\"_input_shape[0]: {_input_shape[0]}\")\n",
    "\n",
    "        self.memory_layer = tf.keras.layers.Embedding(input_dim=_input_shape[0],\n",
    "                                                      output_dim=self.d_model,\n",
    "                                                      trainable=True,\n",
    "                                                      weights=[_mem_matrix],\n",
    "                                                      input_length=self.seq_len)\n",
    "\n",
    "        self.ones_matrix = tf.cast(tf.ones((_mem_matrix.shape[0], self.seq_len)), tf.float32)\n",
    "\n",
    "\n",
    "        self.decoder = Decoder(_key_dim=_key_dim,\n",
    "                               _num_heads=_num_heads,\n",
    "                               _seq_len=self.seq_len,\n",
    "                               _d_model=self.d_model,\n",
    "                               _ffn_units=self.ffn_units)\n",
    "\n",
    "        self.flatten_layer = tf.keras.layers.Flatten()\n",
    "\n",
    "        self.linear = tf.keras.layers.Dense(units=1, activation='linear')  # outputs single neuron from this layer for binary class prediction (instead of seq prediction)\n",
    "        self.sigmoid = tf.keras.layers.Dense(units=2, activation='sigmoid') # outputs single neuron from this layer for binary class prediction (instead of seq prediction)\n",
    "\n",
    "    def create_padding_mask(self, seq): #seq: (batch_size, seq_length)\n",
    "        # Create the mask for padding\n",
    "        mask = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "        return mask[:, tf.newaxis, tf.newaxis, :]\n",
    "\n",
    "    def create_look_ahead_mask(self):\n",
    "        # Create the mask for the causal attention\n",
    "        look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((self.seq_len, self.seq_len)), -1, 0)\n",
    "        return look_ahead_mask\n",
    "\n",
    "    def call(self, _inputs):\n",
    "        tf.print(f\"Pass: {self.count}\")\n",
    "        _input = self.inputs(_inputs)\n",
    "        print(f\"encoder input shape: {_input}\")\n",
    "\n",
    "        _enc_mask = self.create_padding_mask(seq=_input)\n",
    "\n",
    "        _encoder_output = self.encoder(_inputs=_input, _mask=_enc_mask)\n",
    "\n",
    "        print(f\"_encoder_output shape: {_encoder_output.shape}\")\n",
    "\n",
    "        _dec_mask = self.create_look_ahead_mask()\n",
    "\n",
    "        _mem_matrix = self.memory_layer(self.ones_matrix)\n",
    "\n",
    "        _decoder_output = self.decoder(_inputs=_mem_matrix,\n",
    "                                       _encoder_output=_encoder_output,\n",
    "                                       _mask=_dec_mask)\n",
    "\n",
    "        print(f\"_decoder_output shape: {_decoder_output.shape}\")\n",
    "\n",
    "        flattened = self.flatten_layer(_decoder_output)\n",
    "\n",
    "        print(f\"flattened shape: {flattened.shape}\")\n",
    "\n",
    "        _linear = self.linear(flattened)\n",
    "\n",
    "        output = self.sigmoid(_linear)\n",
    "\n",
    "        tf.print(f\"End of Pass: {self.count}\")\n",
    "\n",
    "        print(f\"model output shape: {output.shape}\")\n",
    "\n",
    "        self.count += 1\n",
    "\n",
    "        return output\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def apply_gradients(optimizer, loss, model, labels, _inputs):\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = model(_inputs=_inputs)\n",
    "\n",
    "        loss_val = loss(y_true=labels, y_pred=logits)\n",
    "\n",
    "    gradients = tape.gradient(loss_val, model.trainable_weights)\n",
    "    optimizer.apply_gradients(zip(gradients,  model.trainable_weights))\n",
    "\n",
    "    return logits, loss_val\n",
    "\n",
    "def train_one_epoch(train, train_acc_metric, optimizer, loss, model):\n",
    "    losses = []\n",
    "    pbar = tqdm.tqdm(total=len(list(enumerate(train))), position=0, leave=True, bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt}')\n",
    "\n",
    "    for step, (x_batch_train, y_batch_train) in enumerate(train):\n",
    "        print(f\"step: {step}\")\n",
    "        logits, loss_value = apply_gradients(optimizer=optimizer,\n",
    "                                             loss=loss,\n",
    "                                             labels=y_batch_train,\n",
    "                                             model=model,\n",
    "                                             _inputs=x_batch_train)\n",
    "        losses.append(loss_value)\n",
    "        train_acc_metric(y_batch_train, logits)\n",
    "        pbar.set_description(\"Training loss: %.4f\" % (float(loss_value)))\n",
    "        pbar.update()\n",
    "    return losses\n",
    "\n",
    "\n",
    "def perform_validation(test, model, loss, val_acc_metric):\n",
    "    losses = []\n",
    "    for x_val, y_val in test:\n",
    "        val_logits = model(x_val, y_val)\n",
    "        val_loss = loss(y_true=y_val, y_pred=val_logits)\n",
    "        losses.append(val_loss)\n",
    "        val_acc_metric(y_val, val_logits)\n",
    "\n",
    "    return losses\n",
    "\n",
    "def train_n_epochs(train, test, loss, optimizer, _model, epochs, train_acc_metric, val_acc_metric):\n",
    "    epoch_train_losses, epoch_val_losses = [], []\n",
    "    history = {}\n",
    "    for epoch in range(epochs):\n",
    "        print(f'Epoch: {epoch+1}')\n",
    "        losses_train = train_one_epoch(train=train,\n",
    "                                       train_acc_metric=train_acc_metric,\n",
    "                                       optimizer=optimizer,\n",
    "                                       loss=loss,\n",
    "                                       model=_model)\n",
    "        train_acc_metric.result()\n",
    "        losses_val = perform_validation(test, _model, loss, val_acc_metric)\n",
    "        val_acc_metric.result()\n",
    "        mean_train_loss = np.mean(losses_train)\n",
    "        mean_val_loss = np.mean(losses_val)\n",
    "        epoch_train_losses.append(mean_train_loss)\n",
    "        epoch_val_losses.append(mean_val_loss)\n",
    "        history['epoch' + str(epoch+1) + 'train'] = mean_train_loss\n",
    "        history['epoch' + str(epoch+1) + 'val'] = mean_val_loss\n",
    "        print('\\n Epoch %s: Train loss: %.4f  Validation Loss: %.4f, Train Accuracy: %.4f, Validation Accuracy %.4f' % (epoch, float(mean_train_loss), float(mean_val_loss), float(train_acc_metric.result()), float(val_acc_metric.result())))\n",
    "        train_acc_metric.reset_states()\n",
    "        val_acc_metric.reset_states()\n",
    "    return _model\n",
    "\n",
    "def compile_run_transformer(hyper_param_dict={}):\n",
    "\n",
    "    # default values for hyperparams\n",
    "    _embedding_size=300\n",
    "    _optimizer='adam'\n",
    "    _loss='binary_crossentropy'\n",
    "    _accuracy='binary_accuracy'\n",
    "    _epochs = 1\n",
    "    _batch_size = 64\n",
    "    _activation = 'sigmoid'\n",
    "    _num_heads = 10\n",
    "    _seq_len = 39\n",
    "    input_shape=_x_train.shape\n",
    "\n",
    "    print(f'_x_train.shape: {_x_train.shape}')\n",
    "\n",
    "    # change hyperparams if supplied via hyper_param_dict\n",
    "    if 'tokenizer' in hyper_param_dict:\n",
    "        _tokenizer = hyper_param_dict['tokenizer']\n",
    "    if 'sequence_length' in hyper_param_dict:\n",
    "        sequence_length = hyper_param_dict['sequence_length']\n",
    "    if 'input_shape' in hyper_param_dict:\n",
    "        input_shape = hyper_param_dict['input_shape']\n",
    "    if 'embedding_size' in hyper_param_dict:\n",
    "        _embedding_size = hyper_param_dict['embedding_size']\n",
    "    if 'accuracy' in hyper_param_dict:\n",
    "        _accuracy = hyper_param_dict['accuracy']\n",
    "\n",
    "    if 'optimizer' in hyper_param_dict:\n",
    "        _optimizer = hyper_param_dict['optimizer']\n",
    "    if 'loss' in hyper_param_dict:\n",
    "        _loss = hyper_param_dict['loss']\n",
    "    if 'epochs' in hyper_param_dict:\n",
    "        _epochs = hyper_param_dict['epochs']\n",
    "    if 'batch_size' in hyper_param_dict:\n",
    "        _batch_size = hyper_param_dict['batch_size']\n",
    "\n",
    "    # mask\n",
    "    _y_mask = []\n",
    "    for _i in _y_train:\n",
    "        if _i[1]:  # indicates is spam, '1' in 2nd col indicates spam\n",
    "            _tmp = []\n",
    "            for __i in range(len(_x_train[0])):\n",
    "                _tmp.append(True)\n",
    "            _y_mask.append(_tmp)\n",
    "        else:\n",
    "            _tmp = []\n",
    "            for __i in range(len(_x_train[0])):\n",
    "                _tmp.append(False)\n",
    "            _y_mask.append(_tmp)\n",
    "    _mask = tf.boolean_mask(_x_train, mask=np.array(_y_mask), axis=None, name='boolean_mask')\n",
    "\n",
    "    memory_matrix = np.random.randn(input_shape[0], 300) # uses this instead for input into decoder as we are not predicting sequences, but binary classes\n",
    "\n",
    "    y_train_input = np.zeros((input_shape[0], _seq_len, 2))\n",
    "    y_test_input = np.zeros((_y_test.shape[0], _seq_len, 2))\n",
    "\n",
    "    small_memory_matrix = np.random.randn(20, 10) # uses this instead for input into decoder as we are not\n",
    "\n",
    "    small_y_train = np.random.randn(20, 5, 2)\n",
    "    small_y_test = np.random.randn(5, 5, 2)\n",
    "\n",
    "    small_x_train = np.random.randn(20, 5, 10)\n",
    "    small_x_test = np.random.randn(5, 5, 10)\n",
    "\n",
    "    small_train = tf.data.Dataset.from_tensor_slices((small_x_train[np.newaxis, :, :], small_y_train[np.newaxis, :, :]))\n",
    "    small_test = tf.data.Dataset.from_tensor_slices((small_x_test[np.newaxis, :, :], small_y_test[np.newaxis, :, :]))\n",
    "\n",
    "    small_embeddings = np.random.randn(20, 5, 10)\n",
    "\n",
    "\n",
    "\n",
    "    for i in range(_seq_len):\n",
    "        y_train_input[:, i, :] = _y_train[:, :]\n",
    "        y_test_input[:, i, :] = _y_test[:, :]\n",
    "\n",
    "    print(f\"y_train_input shape: {y_train_input.shape}\")\n",
    "\n",
    "    trfmer = AttentionBasedTransformer(_embedding_size=_embedding_size,\n",
    "                                       _num_heads=_num_heads,\n",
    "                                       _mask = _mask,\n",
    "                                       _key_dim=int(_embedding_size/_num_heads),\n",
    "                                       _batch_size=_batch_size,\n",
    "                                       _input_shape=input_shape,\n",
    "                                       _mem_matrix=memory_matrix,\n",
    "                                       _d_model=300,\n",
    "                                       _seq_len=39)\n",
    "    print(f\"_y_train shape: {_y_train.shape}\")\n",
    "    _train = tf.data.Dataset.from_tensor_slices((_x_train[np.newaxis, :, :], y_train_input[np.newaxis, :, :]))\n",
    "    _test = tf.data.Dataset.from_tensor_slices((_x_test[np.newaxis, :, :], y_test_input[np.newaxis, :, :]))\n",
    "\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    train_n_epochs(_model=trfmer,\n",
    "                   epochs=1,\n",
    "                   loss= tf.keras.losses.BinaryCrossentropy(),\n",
    "                   optimizer=tf.keras.optimizers.Adam(),\n",
    "                   train=_train,\n",
    "                   test=_test,\n",
    "                   train_acc_metric=tf.keras.metrics.BinaryAccuracy(\n",
    "                       name='binary_accuracy', dtype=None, threshold=0.5\n",
    "                   ),\n",
    "                   val_acc_metric=tf.keras.metrics.BinaryAccuracy(\n",
    "                       name='binary_accuracy', dtype=None, threshold=0.5\n",
    "                   ))\n",
    "    \"\"\"\n",
    "\n",
    "    # initialize our TensorBoard callback for better visualization\n",
    "    tensorboard = tf.keras.callbacks.TensorBoard(f\"logs/spam_classifier_{time.time()}\")\n",
    "\n",
    "    # Clear backend\n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "    #\"\"\"\n",
    "    trfmer.compile(\n",
    "        optimizer=_optimizer,\n",
    "        loss=_loss,\n",
    "        metrics=[_accuracy, tf.keras.metrics.Precision(), tf.keras.metrics.Recall()]  # maybe define custom f1 score\n",
    "    )\n",
    "\n",
    "    # train the model\n",
    "    trfmer.fit(_x_train,\n",
    "               _y_train,\n",
    "               validation_data=(_x_test, _y_test),\n",
    "               epochs=_epochs,\n",
    "               callbacks=[tensorboard],\n",
    "               verbose=1)\n",
    "    #\"\"\"\n",
    "\n",
    "    trfmer.summary()\n",
    "\n",
    "    trfmer.save(results_folder / f'trfmer_model_{_optimizer}_opt_{_loss}_loss_{_epochs}_epochs_{_embedding_size}_embedSize_{_batch_size}_batchSize')\n",
    "\n",
    "    return trfmer\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "compile_run_transformer()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}