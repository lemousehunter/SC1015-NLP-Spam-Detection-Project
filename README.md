# SC1015 Natural Language Processing (NLP) Spam Detection Project
An NLP project that uses Classification Trees and LSTM to predict whether a message is spam or ham.
# Welcome!

## About

This is our Mini-Project for SC1015 (Introduction to Data Science and Artificial Intelligence) which focuses on differentiating ham and spam messages from a UCI Machine Learning Repository Dataset (https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection). 

To fully understand our project, please view the source code in order from: (KIV)

1. [tmp1](link)
2. [tmp2](link)
3. [tmp3](link)
4. [tmp4](link)
5. [tmp5](link)
  
## Contributors

- @lemousehunter - Primary Coder, Neural Networks, Machine Learning Engineer
- @raydent30 - Secondary Coder, Exploratory Data Analysis, Data Analytics

## Problem Definition

- How do we differentiate ham and spam messages using machine learning? 
- Which model would be the best to predict it?

## Models Used

1. Binary Classification Tree
2. Long Short-Term Memory (LSTM)

## Conclusion (KIV)

- fwf
- wfaw
- wafwf
- wfa
- Classification trees are able to predict the type of message with high accuracies, but with a low F1 score.
- LSTM performs better than classification trees in predicting the type of message. 
- Yes, it is possible to differentiate ham and spam messages using machine learning, however, there is still room for improvement.

- Popularity and budget have low linear correlation value with ratings (watch out for bandwagons ðŸ¤£)
- Popularity of the casts and crews have higher linear correlation value with ratings
- Resampling imbalanced data improved model performance especially on the minority class
- Logistic Regression did not perform well with non-linearly correlated variables
- Neural Networks along with SMOTEENN resampling method consistently did well in predicting good movies after 100 training attempts (around 72% accuracy, 70% recall)
- Yes, it is possible to predict if a movie is good with acceptable amount of accuracy and recall

## What did we learn from this project?

- Usage of Different Scoring Metrics
- Neural Networks, Keras and Tensorflow
- Concept of Transformers
- Collaborating using GitHub

## References

Dataset:
- <https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection>

Exploratory Data Analysis:
- <https://www.uopeople.edu/blog/punctuation-marks/#:~:text=What%20are%20the%2014%20Punctuation%20Marks%20in%20English%3F,%2C%20quotation%20mark%2C%20and%20ellipsis.>
- <https://www.w3schools.com/tags/ref_urlencode.asp>

RNN & Long Short-Term Memory (LSTM):
- <https://towardsdatascience.com/recurrent-neural-networks-and-natural-language-processing-73af640c2aa1>
- <https://analyticsindiamag.com/lstm-vs-gru-in-recurrent-neural-network-a-comparative-study/>
- <https://medium.datadriveninvestor.com/recurrent-neural-network-58484977c445>
- <https://www.analyticsvidhya.com/blog/2021/03/introduction-to-long-short-term-memory-lstm/>
- <https://medium.com/tech-break/recurrent-neural-network-and-long-term-dependencies-e21773defd92>

Scoring Metrics:
- <https://community.expert.ai/articles-showcase-56/precision-and-recall-f-score-and-accuracy-measuring-nlp-performance-191>
- <https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21>
- <https://www.datarobot.com/blog/introduction-to-loss-functions/>
- <https://medium.com/analytics-vidhya/accuracy-vs-f1-score-6258237beca2>

Transformers:
- <https://www.kaggle.com/rafjaa/resampling-strategies-for-imbalanced-datasets>
- <https://towardsdatascience.com/understanding-encoder-decoder-sequence-to-sequence-model-679e04af4346>
- <https://medium.com/analytics-vidhya/transformer-vs-rnn-and-cnn-18eeefa3602b>
